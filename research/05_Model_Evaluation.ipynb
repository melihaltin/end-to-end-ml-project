{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\development\\\\Machine-Learning\\\\end-to-end-ml-project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\development\\\\Machine-Learning\\\\end-to-end-ml-project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/melihaltin/end-to-end-ml-project.mlflow'\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'melihaltin'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '61b4d7966434a0202f289ca14a1157c66d2b487f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    X_test_data_path: Path\n",
    "    y_test_data_path: Path\n",
    "    model_path: Path\n",
    "    all_params: dict\n",
    "    metric_file_path: Path\n",
    "    target_column: str\n",
    "    mlflow_uri: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_project.constants import *\n",
    "from ml_project.utils.common import read_yaml , create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('config/config.yaml')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath= CONFIG_FILE_PATH , params_filepath=PARAMS_FILE_PATH ,schema_filepath=SCHEMA_FILE_PATH ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        \n",
    "    def get_model_evaluation_config(self):\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.LGBM\n",
    "        schema = self.schema.TARGET\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            X_test_data_path=Path(config.X_test_data_path),\n",
    "            y_test_data_path=Path(config.y_test_data_path),\n",
    "            model_path=Path(config.model_path),\n",
    "            all_params=params,\n",
    "            metric_file_path=Path(config.metric_file_path),\n",
    "            target_column=schema.y,\n",
    "            mlflow_uri=config.mlflow_uri\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return model_evaluation_config\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-26 16:45:28,736: INFO: utils:  Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.]\n",
      "[2024-02-26 16:45:28,736: INFO: utils:  NumExpr defaulting to 8 threads.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import mlflow \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from urllib.parse import urlparse\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def eval_metrics(self, actual, pred):\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        acc = accuracy_score(actual, pred)\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        return rmse, mae, r2, acc\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        X_test = pd.read_csv(self.config.X_test_data_path)\n",
    "        y_test = pd.read_csv(self.config.y_test_data_path)\n",
    "        \n",
    "        model = joblib.load(self.config.model_path)\n",
    "      \n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "\n",
    "         \n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            prediction_qualities = model.predict(X_test)\n",
    "            (rmse, mae, r2 , acc) = self.eval_metrics(y_test, prediction_qualities)\n",
    "            \n",
    "            scores = {\n",
    "                \"acc\": acc,\n",
    "                \"rmse\": rmse,\n",
    "                \"mae\": mae,\n",
    "                \"r2\": r2\n",
    "            }\n",
    "            \n",
    "            save_json(self.config.metric_file_path, data = scores)\n",
    "            \n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            \n",
    "            \n",
    "            mlflow.log_metrics(scores)\n",
    "            # mlflow.log_metrics('mae', mae)\n",
    "            # mlflow.log_metrics('r2', r2)\n",
    "            \n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.sklearn.log_model(model, \"model\" , registered_model_name=\"LGBM\")\n",
    "                \n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\")        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-26 16:48:29,734: INFO: common:  yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-26 16:48:29,736: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2024-02-26 16:48:29,738: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-26 16:48:29,739: INFO: common:  created directory at: artifacts]\n",
      "[2024-02-26 16:48:29,740: INFO: common:  created directory at: artifacts/model_evaluation]\n",
      "https://dagshub.com/melihaltin/end-to-end-ml-project.mlflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\melih\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.4.1.post1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RestException",
     "evalue": "INVALID_PARAMETER_VALUE: Response: {'error_code': 'INVALID_PARAMETER_VALUE'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     model_evaluator\u001b[38;5;241m.\u001b[39mlog_into_mlflow()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     config \u001b[38;5;241m=\u001b[39m ConfigurationManager()\u001b[38;5;241m.\u001b[39mget_model_evaluation_config()\n\u001b[0;32m      3\u001b[0m     model_evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluator(config)\n\u001b[1;32m----> 4\u001b[0m     model_evaluator\u001b[38;5;241m.\u001b[39mlog_into_mlflow()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[20], line 25\u001b[0m, in \u001b[0;36mModelEvaluator.log_into_mlflow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m tracking_url_type_store \u001b[38;5;241m=\u001b[39m urlparse(mlflow\u001b[38;5;241m.\u001b[39mget_tracking_uri())\u001b[38;5;241m.\u001b[39mscheme\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(mlflow\u001b[38;5;241m.\u001b[39mget_tracking_uri())\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m     26\u001b[0m     prediction_qualities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     27\u001b[0m     (rmse, mae, r2 , acc) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metrics(y_test, prediction_qualities)\n",
      "File \u001b[1;32mc:\\Users\\melih\\anaconda3\\envs\\myenv\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:377\u001b[0m, in \u001b[0;36mstart_run\u001b[1;34m(run_id, experiment_id, run_name, nested, tags, description, log_system_metrics)\u001b[0m\n\u001b[0;32m    373\u001b[0m         user_specified_tags[MLFLOW_RUN_NAME] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[0;32m    375\u001b[0m     resolved_tags \u001b[38;5;241m=\u001b[39m context_registry\u001b[38;5;241m.\u001b[39mresolve_tags(user_specified_tags)\n\u001b[1;32m--> 377\u001b[0m     active_run_obj \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_run(\n\u001b[0;32m    378\u001b[0m         experiment_id\u001b[38;5;241m=\u001b[39mexp_id_for_run,\n\u001b[0;32m    379\u001b[0m         tags\u001b[38;5;241m=\u001b[39mresolved_tags,\n\u001b[0;32m    380\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    381\u001b[0m     )\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_system_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# If `log_system_metrics` is not specified, we will check environment variable.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     log_system_metrics \u001b[38;5;241m=\u001b[39m MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[1;32mc:\\Users\\melih\\anaconda3\\envs\\myenv\\Lib\\site-packages\\mlflow\\tracking\\client.py:339\u001b[0m, in \u001b[0;36mMlflowClient.create_run\u001b[1;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_run\u001b[39m(\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    290\u001b[0m     experiment_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m     run_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Run:\n\u001b[0;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    Create a :py:class:`mlflow.entities.Run` object that can be associated with\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    metrics, parameters, artifacts, etc.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m        status: RUNNING\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracking_client\u001b[38;5;241m.\u001b[39mcreate_run(experiment_id, start_time, tags, run_name)\n",
      "File \u001b[1;32mc:\\Users\\melih\\anaconda3\\envs\\myenv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:132\u001b[0m, in \u001b[0;36mTrackingServiceClient.create_run\u001b[1;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Extract user from tags\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# This logic is temporary; the user_id attribute of runs is deprecated and will be removed\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# in a later release.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m user_id \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mget(MLFLOW_USER, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mcreate_run(\n\u001b[0;32m    133\u001b[0m     experiment_id\u001b[38;5;241m=\u001b[39mexperiment_id,\n\u001b[0;32m    134\u001b[0m     user_id\u001b[38;5;241m=\u001b[39muser_id,\n\u001b[0;32m    135\u001b[0m     start_time\u001b[38;5;241m=\u001b[39mstart_time \u001b[38;5;129;01mor\u001b[39;00m get_current_time_millis(),\n\u001b[0;32m    136\u001b[0m     tags\u001b[38;5;241m=\u001b[39m[RunTag(key, value) \u001b[38;5;28;01mfor\u001b[39;00m (key, value) \u001b[38;5;129;01min\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mitems()],\n\u001b[0;32m    137\u001b[0m     run_name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    138\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\melih\\anaconda3\\envs\\myenv\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:178\u001b[0m, in \u001b[0;36mRestStore.create_run\u001b[1;34m(self, experiment_id, user_id, start_time, tags, run_name)\u001b[0m\n\u001b[0;32m    168\u001b[0m tag_protos \u001b[38;5;241m=\u001b[39m [tag\u001b[38;5;241m.\u001b[39mto_proto() \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags]\n\u001b[0;32m    169\u001b[0m req_body \u001b[38;5;241m=\u001b[39m message_to_json(\n\u001b[0;32m    170\u001b[0m     CreateRun(\n\u001b[0;32m    171\u001b[0m         experiment_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(experiment_id),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    177\u001b[0m )\n\u001b[1;32m--> 178\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_endpoint(CreateRun, req_body)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Run\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mrun)\n",
      "File \u001b[1;32mc:\\Users\\melih\\anaconda3\\envs\\myenv\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:59\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[1;34m(self, api, json_body)\u001b[0m\n\u001b[0;32m     57\u001b[0m endpoint, method \u001b[38;5;241m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[0;32m     58\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call_endpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[1;32mc:\\Users\\melih\\anaconda3\\envs\\myenv\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:220\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[1;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[0;32m    218\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[0;32m    219\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n\u001b[1;32m--> 220\u001b[0m response \u001b[38;5;241m=\u001b[39m verify_rest_response(response, endpoint)\n\u001b[0;32m    221\u001b[0m js_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    222\u001b[0m parse_dict(js_dict\u001b[38;5;241m=\u001b[39mjs_dict, message\u001b[38;5;241m=\u001b[39mresponse_proto)\n",
      "File \u001b[1;32mc:\\Users\\melih\\anaconda3\\envs\\myenv\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:152\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[1;34m(response, endpoint)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _can_parse_as_json_object(response\u001b[38;5;241m.\u001b[39mtext):\n\u001b[1;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RestException(json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext))\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m         base_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    155\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != 200\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         )\n",
      "\u001b[1;31mRestException\u001b[0m: INVALID_PARAMETER_VALUE: Response: {'error_code': 'INVALID_PARAMETER_VALUE'}"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager().get_model_evaluation_config()\n",
    "    model_evaluator = ModelEvaluator(config)\n",
    "    model_evaluator.log_into_mlflow()\n",
    "except Exception as e:\n",
    "    raise e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
